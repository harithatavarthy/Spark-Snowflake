{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparkSession_toSnowflake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMX5sdCwrpd0mPOq4V9FWdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harithatavarthy/Spark-Snowflake/blob/master/sparkSession_toSnowflake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nInDG4fpmR1N",
        "colab_type": "text"
      },
      "source": [
        "**The purpose of this notebook is to demonstrate connectivity to snowflake from spark using spark-snowflake connector. For demonstration purpose , we will use Spark 2.3.4 with Hadoop 2.7.  We will also use Snowflake JDBC version 3.6.12 and Spark-Snowflake connector version 2.11-2.4.8. If you are using a different version of spark, you will have to download the appropriate versions of spark-snowflake connector. For more information, refer to snowflake documentation https://docs.snowflake.net/manuals/user-guide/spark-connector-install.html**\n",
        "\n",
        "***You dont have to make changes to any of the cells below except for the part passing snowflake credentials and attributes***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1aKkue0fGSi",
        "colab_type": "text"
      },
      "source": [
        "**Install Open JDK and Spark version 2.3.5 with Hadoop 2.7**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKlDXgVGVkgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCgqQTn1fuxI",
        "colab_type": "text"
      },
      "source": [
        "**Get Snowflake Connector and JCBC Driver**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka_BcOtjdSOV",
        "colab_type": "code",
        "outputId": "4e6b8acb-0d2e-4ae1-c203-3ca55c56643d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "!wget -O snowflake-jdbc-3.6.12.jar https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.6.12/snowflake-jdbc-3.6.12.jar\n",
        "!wget -O spark-snowflake_2.11-2.4.8.jar https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.11/2.4.8/spark-snowflake_2.11-2.4.8.jar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-26 01:39:28--  https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.6.12/snowflake-jdbc-3.6.12.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 151.101.200.209\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|151.101.200.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15462110 (15M) [application/java-archive]\n",
            "Saving to: ‘snowflake-jdbc-3.6.12.jar’\n",
            "\n",
            "\r          snowflake   0%[                    ]       0  --.-KB/s               \rsnowflake-jdbc-3.6. 100%[===================>]  14.75M  89.3MB/s    in 0.2s    \n",
            "\n",
            "2020-01-26 01:39:28 (89.3 MB/s) - ‘snowflake-jdbc-3.6.12.jar’ saved [15462110/15462110]\n",
            "\n",
            "--2020-01-26 01:39:29--  https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.11/2.4.8/spark-snowflake_2.11-2.4.8.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 151.101.200.209\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|151.101.200.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 602469 (588K) [application/java-archive]\n",
            "Saving to: ‘spark-snowflake_2.11-2.4.8.jar’\n",
            "\n",
            "spark-snowflake_2.1 100%[===================>] 588.35K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-01-26 01:39:29 (9.21 MB/s) - ‘spark-snowflake_2.11-2.4.8.jar’ saved [602469/602469]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDLfb3Mf8e8",
        "colab_type": "text"
      },
      "source": [
        "**Check to see if all the necessary JARS are available for use**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8DcsI9thzy",
        "colab_type": "code",
        "outputId": "564f0647-d9c5-43a4-84a8-45d9cfdd2317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "!ls -ltr /content/*.jar"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 15462110 Sep 25  2018 /content/snowflake-jdbc-3.6.12.jar\n",
            "-rw-r--r-- 1 root root   602469 Oct  5  2018 /content/spark-snowflake_2.11-2.4.8.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb706q7LizZn",
        "colab_type": "text"
      },
      "source": [
        "**Set Spark and Java home. Pass Driver Class Path and the location of JARS to Pyspark application**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPctYqxSV6GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.4-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-class-path /content/snowflake-jdbc-3.6.12.jar:/content/spark-snowflake_2.11-2.4.8.jar --jars /content/snowflake-jdbc-3.6.12.jar,/content/spark-snowflake_2.11-2.4.8.jar pyspark-shell\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kmW2zC1P9hr",
        "colab_type": "text"
      },
      "source": [
        "**Create Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecw8RwIVxlWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = (SparkConf()\n",
        "        .setMaster(\"local[*]\")\n",
        "        .setAppName(\"Spar-Snowflake-Connector\")\n",
        "        )\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu3uKPJljbMT",
        "colab_type": "text"
      },
      "source": [
        "**Check to see if spark properties/configuration is as expected**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3AfCBpfNvw3",
        "colab_type": "code",
        "outputId": "fce120b9-ed1d-4e2c-88c2-a932faa06b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\n",
        "print(spark.sparkContext._conf.getAll())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('spark.driver.host', '2df7e04ca0df'), ('spark.driver.extraClassPath', '/content/snowflake-jdbc-3.6.12.jar:/content/spark-snowflake_2.11-2.4.8.jar'), ('spark.repl.local.jars', 'file:///content/snowflake-jdbc-3.6.12.jar,file:///content/spark-snowflake_2.11-2.4.8.jar'), ('spark.jars', 'file:///content/snowflake-jdbc-3.6.12.jar,file:///content/spark-snowflake_2.11-2.4.8.jar'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.app.id', 'local-1580004055747'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.port', '45187'), ('spark.app.name', 'Spar-Snowflake-Connector')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxyOdf0tj1tI",
        "colab_type": "text"
      },
      "source": [
        "**Set options for snowflake connector. Ensure the role you use has privileges to create a stage on the schema being referred to**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoGnnfxU0IjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sfOptions = {\n",
        "  \"sfURL\" : \"accountname.snowflakecomputing.com:443\",\n",
        "  \"sfUser\" : \"username\",\n",
        "  \"sfPassword\" : \"password\",\n",
        "  \"sfRole\" : \"role\",\n",
        "  \"sfSchema\" : \"schema\",\n",
        "  \"sfDatabase\" : \"database\",\n",
        "  \"sfWarehouse\" : \"warehouse\"\n",
        "    }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne8fBtA9kOt9",
        "colab_type": "text"
      },
      "source": [
        "**Enable Query Pushdown  and fire query against snowflake**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9I77VV1D7T",
        "colab_type": "code",
        "outputId": "26fe0d94-4718-467e-ed3a-77083951837d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())\n",
        "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
        "\n",
        "df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option(\"query\",  \"select 1 as mynum union select 2\").load()\n",
        "df.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|MYNUM|\n",
            "+-----+\n",
            "|    1|\n",
            "|    2|\n",
            "+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXenx1InrfKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}